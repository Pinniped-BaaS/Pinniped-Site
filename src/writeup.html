What is Pinniped?</h3>
<p>
    Pinniped is an open-source<em> backend as a service</em>. It provides
essential backend capabilities, including database management API server
functionality, and user authorization. Pinniped provides portability by scaling
down the traditional client/server database and embedding a lightning-fast
SQLite database alongside the server. Pinniped is designed to be user-friendly
and makes extensibility a snap for engineers needing custom functions and
routes. Code to extend Pinniped is written in familiar Javascript, use any NPM
packages to build on top of what is already provided! Tailored for small teams,
front-end engineers, and early-stage startups, Pinniped streamlines application
development by getting any application up and running fast.
</p>
<h3>Backend-As-A-Service</h3>
<p>
    Before we get into what a backend as a service (BaaS) is and how it can help
web developers, it would be helpful to go over how web applications are
typically structured. We’ll also cover some of the challenges associated with
building a web application and what tools exist to address those challenges.
</p>
<h3>3-Tier Architecture</h3>
<p>
    A common way of building web applications has developed over time that
separates the functionality of the application into three tiers. This separation
gives developers an easy way to think about the parts of an application
discretely and is often reflected in the physical configuration of the hardware
as well. The three tiers are:
</p>
<h4>The Data Tier</h4>
<p>
        The Data Tier is the foundational layer responsible for data storage,
retrieval, and management within a web application. It encompasses the database
and data storage solutions that house the application's data, ensuring data
integrity, security, and efficient access to data.
</p>
<h4>The Application Tier</h4>
<p>
        The Application Tier, or Business Logic Tier, serves as the intermediary
that processes user requests, executes business logic, and manages data exchange
between the presentation and data tiers. It handles the application's core
functionality, including data validation, session management, and integration
with other services.
</p>
<h4>The Presentation Tier</h4>
<p>
        The Presentation Tier is the user-facing layer, comprising the web
application's user interface and user experience components. It displays
information to the users and captures their inputs, facilitating any
interactions between the user and the core application.
</p>
<p>
<img src="images/image1.png" width="" alt="alt_text" title="image_tooltip">
</p>
<h4>Challenges of building a web application</h4>
<p>
    There are many challenges associated with building a web application, we’ll
go over some of them below, particularly as they pertain to the challenges a
BaaS is built to address.
</p>
<p>
<strong>Data Tier</strong>
</p>
<p>
        To store the data used by your application you must decide how and where
the data is stored. There are broad choices to make - NoSQL or SQL,
client/server or embedded databases, cloud or local - but also specific
implementations within each category. Whatever you choose, there will be some
amount of configuration specific to that product. Down the road, if the amount
of reads or writes to the data store outgrows its capabilities the architecture
of your database will need to change, the strategy employed will vary depending
on the bottleneck and what kind of traffic your architecture is designed for.
</p>
<p>
        One strategy would be to upgrade the hardware the database is running
on, referred to as vertical scaling. This is often the simplest option for
dealing with bottlenecks as it doesn’t require a change in architecture. Price
is one of the main limitations of this strategy as hardware doesn’t scale
linearly with price and there’s a point where better hardware simply doesn’t
exist.
</p>
<p>
        Strategies for dealing with high load on the database without changing
the database are using caches and queues. By checking a cache to retrieve
commonly requested data before querying the database, read operations on the
database can be reduced. Using a queue to store write operations that the
database pulls from can be a strategy to handle peak traffic and spread the
writes over time. With both these strategies you have to consider the accuracy
of the data being retrieved. For a cache: how long ago was data placed in the
cache and is it still an accurate reflection of the database? For a queue: if
there is a lag between initializing a write operation and it being reflected in
the database, as happens under heavy load, reads that happen while the write is
in the queue will be out of sync.
</p>
<p>
        A set of strategies referred to as horizontal scaling fall into two
categories: replication and partitioning. Replication at its most basic is
maintaining multiple copies of the database on separate machines. This allows
you to spread read operations between copies of the database, reducing the load
on any one copy. The other strategy, partitioning, is breaking a database into
smaller chunks and assigning them to a node (often called sharding). When
partitioning, each node of the database is not a copy of the others, instead
it’s a subset of the overall data. This allows for writing the appropriate data
to each node and increases the overall writes to the database. Partitioning is
an extremely difficult architectural change to implement and maintain and should
be considered only when all other strategies have been exhausted.
</p>
<p>
        Other motivations to horizontally scale the database beyond read/write
bottlenecks include fault tolerance (if there is a hardware failure on one copy
you can recover the data from another copy), increased up-time, and the ability
to physically place copies of the database closer to the end user thus reducing
latency.
</p>
<p>
        Each of these strategies comes with its own set of tradeoffs that must
be carefully considered. As complexity grows so does the difficulty of
maintaining the system and the cost to operate it.
</p>
<p>
        (Kleppmann 144-146)
</p>
<p>
<strong>Application Tier</strong>
</p>
<p>
        The application layer is where the logic of your application lives, what
that logic is depends on the nature of your web application. At the very least,
there needs to be a way to handle incoming HTTP requests to interact with the
database and serve files and webpages.
</p>
<p>
<strong>Hosting</strong>
</p>
<p>
        Hosting can be a complicated part of getting a web application running,
there are technical details of where to host the application, whether locally or
in the cloud, as well as security concerns surrounding the configuration of the
hosting environment and the application. If the hardware is located on-site and
the application experiences large growth, planning and implementing the scaling
of physical architecture is not a simple endeavor.
</p>
<h4>Tools that help solve these challenges</h4>
<p>
    The challenges encountered while building a web application can be addressed
each time a new application is built. However, many tools have been built to
handle parts of the process. A brief overview of common tools and their AWS
implementations will place our project, a BaaS, in a broader landscape.
</p>
<ul>
<li><strong>Infrastructure as a Service - IaaS</strong>
<p>
    This category offers hardware usage over the internet in a pay-as-you-go
style solving the issue of where to physically run the web application and
server. One commonly offered service in this category is a Virtual Private
Server, or VPS, which is a server and operating system on which to host the
application. AWS EC2 instances are a commonly used VPS solution in this
category.
</p>
<p>
    VPS are generally not run on dedicated servers, instead, multiple virtual
servers are run on a single piece of hardware. This allows the provider to make
efficient use of their physical hardware and in many cases ends up being less
expensive for the user of these services as they only pay for what they need,
instead of needing to purchase an oversized piece of hardware capable of
handling peak usage and otherwise being under-utilized.
</p>
<p>
    When developers take advantage of IaaS products, it removes the need to
physically purchase, house, power, and maintain their hardware. This generally
improves reliability and cost. This allows developers to focus on software
concerns, which still include server configurations such as reverse proxies,
firewalls, and load balancers. As well as ensuring the correct runtime
environment and dependencies the application requires are present on the server
and configured properly.
</p>
<ul>
<li><strong>Platform as a Service - PaaS</strong>
<p>
    PaaS takes things a step further than IaaS, providing not only the physical
infrastructure but also taking care of the server configuration. AWS Elastic
Beanstalk is an example in this category which deploys the application,
configures reverse proxies and load balancers, monitors system health, and
ensures the correct runtime environment for the deployed application. From a
developer perspective using a PaaS, the main concern is building and configuring
the application in a compatible manner with the specific PaaS, the deployment of
the application can, in best-case scenarios, consist of uploading the project to
the PaaS and pressing run. Developers to leave server-specific configuration and
security concerns to the PaaS provider and focus almost exclusively on
application development.
</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><strong>Backend as a Service - BaaS</strong>
</li>
</ul>
<p>
<strong>	</strong>Most BaaS are similar to a PaaS but build functionality upon
them.
</p>
<p>
    BaaS create and configure the database and often handle automatically
scaling infrastructure to handle traffic. They abstract communication with the
database by auto-generating an API, often a RESTful one, which allows the user
to modify the structure and content of the database. This removes the repetitive
and tedious task of manually building an API to allow for basic CRUD operations
on the data in the database. This automatically built backend reduces
flexibility - though most BaaS solutions mitigate this by offering some way to
write custom backend functionality and custom routes.
</p>
<p>
    Many BaaS products offer a paid and managed tier that further simplifies the
process of application development by offering hosting with minimal
configuration. AWS Amplify falls in this category, allowing developers to focus
on the front-end application while providing back-end functionality such as file
storage, user authentication, and real-time subscriptions.
</p>
<p>
    From a developer perspective, a BaaS removes many of the tedious and
difficult parts of a backend that most applications have in common. If the needs
of the backend aren’t bespoke, then a BaaS allows the developer to focus on the
business logic and customer-facing functionality of the application.
</p>
<p>
Each of these tools has its place. The more the application being developed fits
the norms, the more likely one or more of these tools can be utilized to save
time and money. However, some of these tools will be too restrictive, when
trading control for convenience, the provider's particular way of operating
becomes more relevant, especially in the realm of PaaS and BaaS. Not all
providers are open-source and as needs change, growing the application in the
desired direction isn’t always possible due to limitations in the technology or
vendor lock-in. As a developer, all of these tradeoffs must be considered and
weighed based on the requirements of the project.
</p>
<h3>Existing BaaS solutions</h3>
<p>
	Now that we’ve covered some of the challenges BaaS address, we can take a look
at a few common products and dig deeper into what they offer, and what they
lack.
</p>
<p>
<img src="images/image2.png" width="" alt="alt_text" title="image_tooltip">
</p>
<ul>
<li>Problems with existing solutions
<ul>
<li>Open source solutions allow you to self host…but painfully
<li>Rigidity - All BaaS limit what you can do with the backend, if you want to
do something not allowed for by how they’ve structured the BaaS you probably
struggle. Many BaaS attempt to mitigate this by offering a variety of
extensibility options.
<li>Extensibility using cloud functions can be tricky to configure, cold starts,
latency
<li>Vendor lock-in - which makes it difficult to switch to another BaaS service
without significant migration efforts.
<li>Security & shared infrastructure - you’re inevitably hosting your BaaS on a
machine with other apps, which can pose security concerns.
<li>Cost - you’re at the mercy of the provider’s pricing structure, which is
typically based on usage. This can lead to unpredictable costs for your
business.
<li>Provider dependency - managed services inevitably require network calls. We
know the network can experience disruptions - if the provider is experiencing
disruptions, you’re at their mercy for fixing, and your app may be impacted or
down during this time.
</li>
</ul>
</li>
</ul>
<h2>Pinniped</h2>
<h3>Use Case</h3>
<p>
Pinniped provides essential backend capabilities, including database management
and API server functionality, designed to streamline application development.
Tailored for smaller teams, front-end engineers, and early-stage startups,
Pinniped emphasizes portability, extensibility, and user-friendliness in
delivering a versatile backend solution for small-scale applications.
</p>
<h3>Goals</h3>
<p>
For more context, here’s how Pinniped accomplishes and defines its goals.
</p>
<p>
<strong>Portability</strong>
</p>
<p>
Pinniped defines portability as how easy it is to move the application to any
environment and run as intended. Pinniped achieves this by being easy to deploy
and run on any environment where Node is installed. Pinniped makes the migration
of development changes pushed to a production environment simple.
</p>
<p>
Visual showing x, y, z, associated with portability
</p>
<p>
<strong>Extensibility</strong>
</p>
<p>
Extensibility, in Pinniped’s terms, is how much backend functionality can be
built before needing to switch to a backend that better suits the user’s
requirements. Pinniped's functionality is extendable within the backend's source
code with JavaScript. As a result, Pinniped supports up-to-date JavaScript ES6
features and Node APIs. Although serverless functions are the industry standard,
serverless functions have innate issues of increased network latency and
configuration complexities.<sup id="fnref1"><a href="#fn1"
rel="footnote">1</a></sup> These issues undermine Pinniped’s goals, which
influenced Pinniped to focus on supporting JavaScript natively.
</p>
<p>
<strong>User-Friendly</strong>
</p>
<p>
As straightforward as it sounds, Pinniped’s goal of user-friendliness is based
on how simple and easy it is to interact with and integrate Pinniped into the
application. Users of Pinniped have access to simplified pipelines that can
create a Pinniped-powered application, deploy that application onto a virtual
private server (VPS), and manage that application through an admin dashboard.
</p>
<p>
<img src="images/image3.png" width="" alt="alt_text" title="image_tooltip">
</p>
<h2>Pinniped Architecture</h2>
<p>
Intro - you want Pinniped to be portable, able to run on a single machine
without worrying about configuring multiple components or networking between
them.
</p>
<h4>Strategies of Consolidation</h4>
<p>
Docker Image
</p>
<p>
An initial approach involved creating a base image to install Pinniped and
execute the application. Also, this method requires the user to mount a volume
for database persistence. While Docker offers versatility and allows horizontal
scalability, integrating Pinniped via Docker imposes a steeper learning curve on
users, detracting from the overall user experience. Moreover, the benefits of
horizontal scalability are marginal, as Pinniped's use case would not benefit
from horizontal scaling.
</p>
<p>
npm Package
</p>
<p>
The idea to develop Pinniped as an npm package stemmed from the widespread
familiarity among JavaScript developers with Node.js, the most mature JavaScript
runtime. Consequently, the intended audience for Pinniped would possess the
requisite knowledge to use npm, the default package manager for Node.js. While
both Docker and npm promote portability, npm's simplicity makes it particularly
accessible. Moreover, given the prevalence of npm packages in JavaScript
applications, integrating Pinniped as another package would be straightforward.
Nevertheless, this method has its drawbacks. npm packages potentially present
security risks, and packages can struggle with version control, leading to
conflicts that disrupt functionality. Moreover, an over-reliance on many
packages can induce significant performance overhead costs.
</p>
<p>
Executable File
</p>
<p>
One approach was for Pinniped to compile into an executable file. Once the user
was satisfied with the current state of their application, they could compile
and upload the file to their production environment. This executable would be
injected with a binary large object (BLOB) containing Node, enabling users to
execute their applications seamlessly across varied environments, regardless of
whether Node was present. The main benefit of this approach is portability.
However, popular options for compiling were either deprecated<sup id="fnref2"><a
href="#fn2" rel="footnote">2</a></sup>, incompatible with other dependencies, or
incomplete<sup id="fnref3"><a href="#fn3" rel="footnote">3</a></sup>. The
decision to compile was forgone, as it limited the use of other dependencies
which limited Pinniped’s extensibility.
</p>
<p>
After consideration of the alternative strategies, the npm package approach best
fits the goals of Pinniped without severe negative consequences. The
standardized use of npm makes Pinniped accessible to the majority, if not all,
of JavaScript developers. Although Docker can automate tedious workflows, it
requires a certain level of user proficiency. Compiling executables poses
compatibility challenges with some dependencies, constraining Pinniped's
extendability. Despite npm's potential issues, they do not conflict with
Pinniped's goals.
</p>
<p>
The selection of npm packages warrants conversations about the database and the
application logic. Traditional backends separate the application logic from the
database. But for Pinniped, what is deemed portable relies on the compactness of
the backend. Pinniped needs to stay portable and user-friendly, which becomes
difficult based on conventional architecture. How can Pinniped be installed as
an npm package and bypass the needed setup between the database and API? Well,
Pinniped tries to combine the database and application logic into one Node
process.
</p>
<h4>An Embedded Database</h4>
<p>
Two Tier Architecture
</p>
<p>
Contrary to most backends, Pinniped's architecture follows a two-tier model. A
two-tier architecture model is defined by two functional tiers: a client tier
and a database tier. Instead of a differentiated application tier in the
conventional three-tier model, the application logic can reside anywhere within
the architecture.<sup id="fnref4"><a href="#fn4" rel="footnote">4</a></sup> In
Pinniped's case, the database and the application server live on the same
infrastructure.
</p>
<p>
<img src="images/image4.png" width="" alt="alt_text" title="image_tooltip">
<img src="images/image5.png" width="" alt="alt_text" title="image_tooltip">
</p>
<p>
<img src="images/image6.png" width="" alt="alt_text" title="image_tooltip">
</p>
<p>
“For the sake of our use case, we’ve decided to create Pinniped as a single
entity.”
</p>
<p>
“What does Pinniped’s architecture look like now that it’s compacted into one
Node process?”
</p>
<h3>Embedded Database</h3>
<p>
“Since Pinniped is a single entity, what does that mean for our database?”
</p>
<p>
“What are the differences between a traditional database and an embedded one?”
</p>
<p>
“With this approach, what gets sacrificed and what is provided?”
</p>
<ul>
<li>Relational Database Options - Embedded vs client/server
<ul>
<li>Visual - Embedded Vs Client/Server
<li>Type - Static Table Diagram
<li>Content
<ul>
<li>TBD
</li>
</ul>
<li>Sqlite (embedded)
<ul>
<li>Single file based. Simplicity
<li>Backups are easy to make (can mutate database while creating backup) and to
move/store as the are a simple file
<ul>
<li>Needs more research about traditional client/server databases and how it
contrasts with embedded databases.
</li>
</ul>
<li>Speed is faster than client-server database (because file based/no
traditional connection)
<li>Importantly, a SQLite database can be stored and accessed in the same
location as the web / app server. This allows us to keep the footprint and
complexity of the project down.
</li>
</ul>
<li>Postgres, mysql (client/server), etc
<ul>
<li>client/server based
<li>Lends itself to running the database on a dedicated node/machine
<li>Strong access control
</li>
</ul>
<li>What we chose and why
<ul>
<li>As we looked at the options for a relational database, a non-standard
solution presented itself: SQLite.
<ul>
<li>It would allow us to easily bundle the database with our backend
application.
<li>For single database application, performance is better than a client/server
<li>Setup and configuration is simpler.
</li>
</ul>
<li>Some of the downsides of using SQLite:
<ul>
<li>Security concerns of running database on same node/file-system as server
<ul>
<li>Describe how security concerns could be mitigated/What we do to mitigate the
risk.
</li>
</ul>
<li>Lack of natively supported replication - offset by SQLites extremely fast
reads and that our use case doesn’t cover enterprise applications.
<li>It’s light on validation and access control
<li>Lack of horizontal scalability - those who need this shouldn’t/wouldn’t be
using Pinniped in the first place.
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>
				Trade off between the sqlite lack of strong access and type controls and
maintaining a relational database with some type checking.
</p>
<h4>Database Selection - Important Database Characteristics That Were
Considered</h4>
<h5>Intro </h5>
<p>
When we get here we’ve discussed embedded already
</p>
<h5>Relational vs document databases</h5>
<p>
        When deciding what database to use we considered two categories:
relational and document databases. The major relevant difference between these
databases to our use case is their data model: how the database represents the
data it stores. Relational databases use tables with columns that constrain what
data can be placed in that column and a row for each record in the table.
Document databases represent the data as a tree of one-to-many relationships,
JSON is used by many document databases to structure the stored data. The
differences in how this data is modeled has broad implications for how an
application interacts with the database.
</p>
<p>
        Relational databases are often characterized by their strict schema. The
rules for what types of data can be placed in each table are dependent on the
rules set when the table is created. There is no nested data in the relational
model, all the data is laid out and accessible in the rows of the tables, links
between rows can be created called relationships and is how hierarchical
relationships that are modeled. This lends itself to representing many-to-one
and many-to-many relationships nicely. It also leads to one of the relational
databases' strengths: joins - the ability to select and join rows based on any
arbitrary condition. The rigid structure also allows for building indexes for
common queries, much like the index of a book, it allows the database to quickly
find rows based on the values of a column.
</p>
<p>
        The downsides of relational databases are that the strict schema limits
the variety of data structures that can be easily stored in the database. The
representation of data in tables is often at odds with the way an application
represents data in its code, leading to the need to program an awkward
translation layer between how the data is represented in the application and how
it is stored in the database. The adherence to a strict schema also complicates
making changes to the schema once there is data in the tables, if the type of a
column changes, the data for each record also has to change types to match the
column.
</p>
<p>
</p>
<p>
        In contrast to relational databases, document databases are much less
strict about their schema. Although you can set schema rules in many document
databases, they excel in large part because of the flexibility they offer. The
data doesn't have to conform to the schema before being stored - no matter how
it is structured. Another benefit is that for many applications the
representation of data in the application can closely match its representation
in the database. Another benefit is improved performance, when a document is
retrieved it generally has the information needed in one query contrasted with
the multiple queries needed by a relational database to gather all the
information in a similar situation.
</p>
<p>
    (Kleppmann #38-41)
</p>
<p>
        While document models excel in the context of one-to-many relationships,
they struggle when it comes to many-to-one relationships. Join queries are often
poorly supported or require the application to make multiple queries and stitch
the information together. Document databases are also less suited to
many-to-many relationships as compared to relational databases for similar
reasons.
</p>
<p>
        (Kleppman #34)
</p>
<h5>Our Choice </h5>
<p>
		NOTES
</p>
<p>
		Why SQL vs Document?
</p>
<p>
			Document is more flexible
</p>
<p>
				Flexibility can be good, but database design still needs to be considered as
data grows. How do you search through it? How do you organize it? Document
databases put the onus on the user, it sounds great to say “no schema”, but you
probably still need some structure to the fields. You still need to check
inputs. Migrations seem like a big problem, but once implemented, the rigidity
helps organize and standardize the dataset. It’s the upfront cost of making
things rigid vs the ongoing cost of a) implementing schema in a document
database which happens often enough that major document databases support it. b)
trying to manage the growing complexity manually as the data set grows.
</p>
<p>
The argument for document database flexibility is a bit weak to me, we can store
JSON documents in sqlite, so if the user definitely needs that flexibility, they
have it. Sql databases are old, well established and familiar to most
developers.
</p>
<p>
</p>
<p>
        The question of which database type to use hinges on the needs of the
application. While arguments can be made for either relational or document
databases, for Pinniped, having chosen to embed the database, non-standard for a
BaaS, we had a few main concerns: speed, reliability, documentation, and library
support. After researching our options we landed on SQLite. SQLite is a mature,
widely used, extremely fast SQL implementation with plenty of documentation and
supporting libraries. It met our criteria and had been successfully used as the
database of PocketBase, which was a point in its favor. Without a compelling
reason to seek out a document database alternative, we moved forward with
SQLite.
</p>
<ul>
<li>Transition: Choosing SQLite accomplished our goals to pick an embedded
database with wide support but brought with it additional issues, primarily
around DDL/schema management.
<h4>
        How we handled schema</h4>
<ul>
<li>Intro - We need a programmatic layer because SQLite is light - some data
types and constraints are not supported in SQLite and we needed some way to
handle complex api rules.
</li>
</ul>
</li>
</ul>
<ul>
<li>Sqlite_master vs custom implementation (tablemeta)
<ul>
<li>SQLite master doesn’t do enough - needed specific types added to cols, api
rules, table types (auth v. base) - evidence & example
<ul>
<li>We considered using SQLite’s built-in sqlite_master to introspect the
database and manage schema from there, but because we needed more control (As
mentioned above), we decided to create our own table within our database
(tablemeta).
<li>Additionally, making changes to the SQLite schema would’ve added a lot of
complexity without gaining much (since it’s so light to begin with - even with
changing validation rules, etc. it wouldn’t have done as much as a typical SQL
db would)
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>Knex  + Our custom model Classes vs ORM
<ul>
<li>ORM is too rigid - would have to use their formats for schema changes, which
didn’t work well within our data structure we established
<ul>
<li>We considered using an ORM, but since we were already handling schema
changes programmatically, we landed on less abstraction to allow for the
flexibility we needed to interact with this data structure. Knex is a query
builder that gave us the tools we needed to accomplish that. Needs more research
<ul>
<li>The Solution
<ul>
<li>The combination of tablemeta and Knex allowed us to introduce a programmatic
layer of schema management. This layer consists of an object with: name, type,
rules, and an array of columns.  The data structure is enhanced by 2 classes:
table and column.
<li>Table makes up the data structure, where its columns property is made up of
instances of the Column class.
<li>Each column class has standard validation rules you’d typically see within a
sql schema (required, unique, etc.) and a type (text, number, etc.) with its own
custom validation rules (similar to CHECK). 50/50 sqlite and what our
application needs.  Conditionally determines what data type is created in SQLite
table and also determines how to programmatically validate data as it moves
through the system.
<li>Because of the rigidity of this new data structure for managing schema, it
made sense to introduce an Admin UI to ensure that schema changes followed this
“contract” as specified by the data structure.
<ul>
<li>Migrations
<ul>
<li>Assure that any two databases that run the same set of migrations will have
consistent schema. Allows for schema changes to be brought from dev to prod.
<li>Discuss different migrations strategies
<ul>
<li>migration created by the application apply to the database
<li>Database changes made by the application are introspective to make the
migrations files
</li>
</ul>
<li>We chose to make migrations the source of truth - option 1
<li>Because our schema is meant to be adjusted programmatically (whether through
the admin UI or another REST client), we needed to generate the migration files
automatically.
<li>All occur in transactions
<ul>
<li>Call out to Admin UI and how and why we focused on it and feature it so
heavily in how a user is meant to interact with Pinniped’s schema. (Fancy aside)
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>Visual - How schema works
</li>
</ul>
<li>Type - Animation
<li>Content
<ul>
<li>The req body - bland key pairs sent in the request body - starts here
</li>
</ul>
<li>Table instance is created Show the Table instance, data structure
<ul>
<li>Column instances are created
</li>
</ul>
<li>Migration file is created that does the following bundled operations
<ul>
<li>Tablemeta row is created / updated
<li>pnpd.db table sqlite schema is updated
</li>
</ul>
<li>Migration is executed
<ul>
<li>Transition: Now that we had our tablemeta table, we could leverage it to
greatly streamline the auto generation of our API.  All we had to do was add a
middleware to grab the associated table from tablemeta, and it would serve as
our source of truth for schema/validation and authorization.
</li>
</ul>
<h3>Request/Response Flow</h3>
<h4>
    Intro</h4>
<ul>
<li>Set the stage a bit in this intro, the app starts up, all generic CRUD
routes are there, when a specific table gets hit (which we can find from hitting
the tablemeta route) the route handler verifies that the requested table exists
and we go from there…
<li>With Pinniped's core architecture and schema management strategy in place,
we can now dive into how Pinniped handles API Auto generation, Authorization,
and Extensibility within the request response cycle.
</li>
</ul>
<h4>
    	API Auto Generation</h4>
<ul>
<li>Intro - Our tablemeta table provides us, not only with a layer of schema
management, but also with a single source of truth for: authorization and CRUD
endpoints.  The request/response flow all starts with a ‘LoadTableContext’
middleware which grabs the tableId from req.params and queries for the
associated table within tablemeta.  This provides us with the table’s:
<li>
<li>Name,
<li>Type,
<li>API rules,
<li>And schema, in the form of an array of columns.
<li>Visual - Request Response Flow - API Auto generation focus
<ul>
<li>Type - Animation / FlowChart
<li>Content
<ul>
<li>Process flow showing a request coming in, loadTableContext querying from
tableMeta using the Id in the request path, creating a Table instance, and
passing that down for use in the various route handlers.
</li>
</ul>
</li>
</ul>
<li>We then use this table to pass either its id or name along to our DAO layer
to make queries to the appropriate table
<li>By using this LoadTableContext middleware, we avoid the need to hardcode
routes for every single table, which would be burdensome when tables were
added/dropped, as we would need to rerun the auto generation of those routes
specific to the newly created table.
<li>We create a Create, Read, Update, and Delete route, just once…
<h5>
    Intro</h5>
<p>
        Creating a REST API layer to enable CRUD operations on a static set of
tables is a straightforward, repetitive, tedious process that many developers
are familiar with. BaaS solutions automate this process for developers by
automatically generating an API layer that reflects the schema of the
application.
</p>
<h5>
        Static REST API Layer Creation</h5>
<p>
        Creating a REST API for single table generally involves registering a
standard set of
</p>
<p>
        *
</p>
</li>
</ul>
<p>
</p>
<p>
		Pinniped’s approach
</p>
<p>
</p>
<ul>
<li>Visual - Dynamic CRUD Route Code Snippets
<ul>
<li>Type - Code Snippets
<li>Content
<ul>
<li>Show the registration of CRUD API for dynamic use with any underlying schema
<li>Contrast with creating a non dynamic api, Or iterating over every collection
and creating routes for each
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4>
    Authorization</h4>
<ul>
<li>Intro - Now that we have the API rules for the associated table, we can
compare these rules to the current session’s user profile and determine what is
allowed.
<li>Authentication with sessions (single instance focused use case)
<li>Authorization roles
<ul>
<li>Public, user, creator, admin details
</li>
</ul>
<li>Authorization implementation through middleware for various routers / routes
<ul>
<li>Now that we have the API rules for the associated table, we can compare
these rules to the current session’s user profile and determine what is allowed.
<ul>
<li>401 errors and 403 errors where appropriate
<h4>
    	Extensibility </h4>
<ul>
<li>Intro - Within the req res cycle, we also allow developers to register
additional non crud routes, and register event handler functions that will be
executed when things events are fired by the API.
<li>Local extensibility VS cloud functions
<ul>
<li>Latency
<li>Cold starts
<li>Scalability
<li>Familiar nodejs execution env vs tricky cloud envs
</li>
</ul>
<li>Visual - Local vs cloud function
<ul>
<li>Type - Comparison table
<li>Content
<ul>
<li><a
href="https://www.notion.so/Explicitly-outline-extensibility-benefits-03b85ecb41694d86ab69acd6fd956053">https://www.notion.so/Explicitly-outline-extensibility-benefits-03b85ecb41694d86ab69acd6fd956053</a>
</li>
</ul>
</li>
</ul>
<li>What we picked and why
<ul>
<li>Local extensibility written into each individual project’s code (index.js
and other files / dependencies used by it) using familiar NodeJS and javascript.
<li>Avoids context switching between different execution environments /
languages + other things from above.
<ul>
<li>How Pinniped does local extensibility
<ul>
<li>When you invoke the `pinniped.createApp` method inside index.js, a new
instance of the `pinniped` class is created. This configures the server, makes a
database connection, and ensures db is set up correctly.
<li>After the app instance is created, you can then register custom routes and
event handlers to set up all the custom backend functionality you need.
<ul>
<li>Event based Extensibility
<ul>
<li>Events/node’s EventEmitter
<ul>
<li>Node has a built-in EventEmitter class that made perfect sense for us…
<li>Pinniped class has “onGetallRows” etc… methods that allow users to register
event handler functions to be executed things happen
<ul>
<li>List of supported events
</li>
</ul>
<li>You register event handlers in your index.js file, using the methods on the
pinniped class to specify the event type, the tables that the event should apply
to, and the event handler function to invoke when the event occurs
<ul>
<li>Within the event handler function that you register, you’ll have access to
an `event` parameter that gives you access to the express `req`, and `res`
objects, along with the relevant rows from the table you were interacting with.
</li>
</ul>
<li>Within each route handler, we collect the relevant information that the
event handler might need access to, bundle it into an object, and then fire the
appropriate event, passing the event object to any registered event handlers.
This triggers the execution of any event handlers registered for that event
type.
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>Custom Route extensibility
<ul>
<li>Pinniped class an `addRoute` method that allows you to register additional
routes that you might need for various, “non-crud” reasons.
<li>From the index.js file, you invoke the `addRoute` method on your instance of
the pinniped class, passing in a request method, path, and handler function as
arguments
<ul>
<li>Within that handler function, you’ll have access to the express `req` and
`res` object just like you would inside of any normal route handler function.
<ul>
<li>Finally, once your backend is configured how you’d like with any needed
extensibility, you run the `start` method on the pinniped class to set up the
express server with the auto generating crud api, and custom routes that you’ve
registered.
<ul>
<li>Visual - Show the flow of Event driven extensibility
<ul>
<li>Type - Animation / Flow Chart
<li>Content
<ul>
<li>Request comes in and hits a crud route
<li>Crud route does it’s stuff
<li>Packages up an “event” object,
<li>Fires the event and passes the event object
<li>Event handler receives event object + Req and Res and can do anything with
it.
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3>Deployment / Hosting</h3>
<ul>
<li>It works on any VPS running nodejs
<ul>
<li>Express / No reverse proxy by default for small fast deployment
<ul>
<li>Chosen for simplicity and portability.
<li>Can give a domain for auto cert or setup Pinniped behind a reverse proxy of
your choice.
</li>
</ul>
<li>Auto Cert SSL setup
<ul>
<li>Allows users to use their choice of domain registrar.
<li>Starts server to get cert and then brings up main app with cert.
<li>Attempts auto-renewal on schedule.
<ul>
<li>CLI AWS Automation
<ul>
<li>Intro -  While pinniped will work with in any local, or cloud environment,
to facilitate ease of use, we built out a hosting / deployment pipeline to
deploy a pinniped application to AWS EC2
<li>Provisioning EC2 instances - AWS Sdk V3
<ul>
<li>Prompts for region and instance type
<ul>
<li>Differences between t2 and tg4 instance types and why we picked them
</li>
</ul>
<li>Creates a “Pinniped-Security” security group
<ul>
<li>Ingress rules to allow ssh, http, and https traffic into the instance
</li>
</ul>
<li>Finds AMiId based upon selected instance type
<ul>
<li>Chose ubuntu 22.04 Jammy
</li>
</ul>
<li>Launches the instance
<li>Instance Setup
<ul>
<li>Installs Node, PM2, libcapbin
<li>Setcap command to give ubuntu user access to run node applications on port
80 and 443
</li>
</ul>
</li>
</ul>
<li>Communicating with EC2 instances - SSH
<ul>
<li>Connects via ssh to allow easy file transfer through sftp for syncing local
project directory and EC2 instance server directory
<ul>
<li>Uses of `<code>ssh2`</code> and `ssh2-sftp-client` packages
</li>
</ul>
<li>Allows running of commands remotely to install software, change permissions
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>Using Pinniped (Ease of use)</h2>
<p>
	Creating a project
</p>
<ul>
<li>Short Video - demonstrates the CLI’s create command process.
</li>
</ul>
<h3>Building an Application</h3>
<ul>
<li>Admin UI, Creating Schema, CRUD Operations
<ul>
<li>Short Video - Admin UI Operations
</li>
</ul>
<li>Custom Routes and extensibility
<ul>
<li>Show off example usage
<ul>
<li>SSE
<li>Cron jobs
<li>Payment integration
<li>File storage
<li>Enriching Response through use of additional API’s / dependencies
</li>
</ul>
</li>
</ul>
<li>Building a front-end using the SDK
<li>Hosting a frontend application in dist - or anywhere else
</li>
</ul>
<h3>Flow from Dev to Prod</h3>
<ul>
<li>Provision and Deploy
<ul>
<li>Short Video - Provision and Deploy Commands
</li>
</ul>
<li>Updating
<ul>
<li>Backend and frontend updates
</li>
</ul>
<li>Database backups
<li>Migrations
</li>
</ul>
<h2>Future Work</h2>
<h3>Testing</h3>
<h3>CLI support for more cloud providers</h3>
<h3>Database replication</h3>
<p>
	File Storage
</p>
<h2>Citations</h2>
<p>
Work Cited
</p>
<p>
    Kleppmann, Martin. <em>Designing Data-intensive Applications: The Big Ideas
Behind Reliable, Scalable, and Maintainable Systems</em>. O'Reilly, 2017.
</p>
<h2>References</h2>
<!-- Footnotes themselves at the bottom. -->
<h2>Notes</h2>
<div class="footnotes">
<hr>
<ol><li id="fn1">
<p>
     <a
href="https://www.cloudflare.com/learning/serverless/why-use-serverless/">https://www.cloudflare.com/learning/serverless/why-use-serverless/</a>&nbsp;<a
href="#fnref1" rev="footnote">&#8617;</a><li id="fn2">
<p>
     <a href="https://github.com/vercel/pkg">https://github.com/vercel/pkg</a>&nbsp;<a
href="#fnref2" rev="footnote">&#8617;</a><li id="fn3">
<p>
     <a
href="https://nodejs.org/api/single-executable-applications.html#single-executable-applications">https://nodejs.org/api/single-executable-applications.html#single-executable-applications</a>&nbsp;<a
href="#fnref3" rev="footnote">&#8617;</a><li id="fn4">
<p>
     <a
href="https://www.geeksforgeeks.org/difference-between-two-tier-and-three-tier-database-architecture/">https://www.geeksforgeeks.org/difference-between-two-tier-and-three-tier-database-architecture/</a>&nbsp;<a
href="#fnref4" rev="footnote">&#8617;</a>
</ol></div>